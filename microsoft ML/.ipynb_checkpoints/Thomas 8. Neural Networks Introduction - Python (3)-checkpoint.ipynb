{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Exercise 8 - Introduction to Neural Networks\n=======\n\nOriginally hypothesised in the 1940s, neural networks are now one of the main tools used in modern AI. Neural networks can be used for both regression and categorisation applications. Recent advances with storage, processing power, and open-source tools have allowed many successful applications of neural networks in medical diagnosis, filtering explicit content, speech recognition and machine translation.\n\nIn this exercise we will look at a dataset comparing three different dog breeds, with the age, weight, and height of multiple individuals, and we will attempt to create a neural network model for the purpose of classification, predicting the breed of dog for a given sample individual."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Sets up the graphing configuration\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as graph\n%matplotlib inline\ngraph.rcParams['figure.figsize'] = (15,5)\ngraph.rcParams[\"font.family\"] = 'DejaVu Sans'\ngraph.rcParams[\"font.size\"] = '12'\ngraph.rcParams['image.cmap'] = 'rainbow'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 1\n------\n\nLet's start by opening up our data and having a look at it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport keras\nprint('keras using %s backend'%keras.backend.backend())",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "keras using tensorflow backend\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\n# Loads the dataset\ndata = pd.read_csv('Data/dog_data.csv')\n\n###--- WRITE print(data.head()) TO VIEW THE TOP 5 DATA POINTS OF THE DATA SET ---###\n\n###\n\n# Defines the feature dataframe\nfeatures = data.drop(['breed'], axis = 1)",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "    age  weight  height  breed\n0  9.47    6.20    6.80      1\n1  7.97    8.63    8.92      0\n2  9.51    6.40    5.78      1\n3  8.96    8.82    6.28      2\n4  8.37    3.89    5.62      1\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 2\n\nOur target data, three breeds of dogs, are represented numerically in our dataset, as `0`, `1`, and `2`. But for a neural network such as this, these numbers are a little misleading, it might imply that `1` is closer to `2` than `0` is, in some way. But that is not necessarily the case. Therefore, we may want our network to have three output nodes, one for each of the target classes. For example, given three output nodes, the first node might return positive if our network thinks a sample is of class `0`, while the other two nodes remain silent, or the second node might return positive if it thinks a sample is of class `1`, and so on.\n\nTo this end, we might wish to represent our data using one-hot vectors, that is, representing our target as binary vector combinations with just a single `1` value and all the others `0`.\n\nThis will give us the following target vectors:\n\n| breed 0 | breed 1 | breed 2 |\n|:------- |:------- |:------- |\n| `0 0 1` | `0 1 0` | `1 0 0` |\n\nEach of the three values in our new target vectors will correspond to one of our output nodes, once we set up our network."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import OneHotEncoder\n\n# Sets the  labels (numerical)\nlabels = np.array(df['breed'])\n\n###--- REPLACE THE ??? BELOW WITH labels ---###\nonehot = OneHotEncoder(sparse = False).fit_transform(np.transpose([???]))\n###\n\nprint(onehot[:5])",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 3\n\nFor our model, let us first define our `train_X`, `train_Y`, `test_X`, and `test_Y` variables."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Take the first 4/5 of the data and assign it to training\ntrain_X = features.values[:160]\ntrain_Y = onehot[:160]\n\n# Take the last 1/5 of the data and assign it to testing\ntest_X = features.values[160:]\ntest_Y = onehot[160:]",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 4\n\nLet's start by establishing the model. Sequential is the standard model type for Keras."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Set a randomisation seed for replicatability.\nnp.random.seed(6)\n\n###--- REPLACE THE ??? BELOW WITH Sequential() ---###\nmodel = keras.models.???\n###",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's now add our layers to the model. Our first will be our hidden layer, which will also specify the dimension of the implicit input layer it receives. The number of units in the hidden layer is determined through a number of varying wisdoms. Some such wisdoms are:\n> \"A rule of thumb is for the size of this [hidden] layer to be somewhere between the input layer size ... and the output layer size ...\" (Blum, 1992, p. 60).\"\n   \n> \"Typically, we specify as many hidden nodes as dimensions [principal components] needed to capture 70-90% of the variance of the input data set.\" (Boger and Guterman, 1997).\"\n   \n> \"One rule of thumb is that it should never be more than twice as large as the input layer.\" (Berry and Linoff, 1997, p. 323).\"\n\nSometimes to follow one wisdom, we have to ignore another. For now, however, let's give **two hidden layers, one with 4 nodes and one with 2 nodes**, a go.\n\nMost of our model parameters around this are fairly straightforward, we have an **input dimension of 3** (our three features), and an **output dimension of 3** (our three classes), the output dimension being used in our final layer, the output layer."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# We can use a structure with the format [input nodes, hidden1 nodes, hidden2 nodes, output nodes] to help us\n\n###--- REPLACE THE ???s BELOW WITH THE APPROPRIATE NUMBERS OF NODES ---###\nstructure = [???, ???, ???, ???]\n###\n\n# Hidden layer 1 + input layer\nmodel.add(keras.layers.Dense(units=structure[1], input_dim = structure[0], activation = 'relu'))\n\n# Hidden layer 2\nmodel.add(keras.layers.Dense(units=structure[2], activation = 'relu'))\n\n# Output layer\nmodel.add(keras.layers.Dense(units=structure[-1], activation = tf.nn.softmax))",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 5\n\nWe can now compile and fit our model. We must specify the loss function, the omtimisation function, and our training evaluation metric. For our loss function, we can use *categorical cross entropy*, but the mathematics of that are beyond the scope of this exercise, all we need to know is that it's looking at multiple categories. For our optimizer, we can use *stochastic gradient descent*, as introduced in the course. Lastly, our metric can simply be *accuracy*."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Let's compile the model\n\n###--- REPLACE THE ???s BELOW WITH 'categorical_crossentropy', 'sgd', AND THEN 'accuracy' (INCLUDING THE QUOTES) ---###\nmodel.compile(loss = ???, optimizer = ???, metrics = [???])\n###\n\n# Time to fit the model\nprint('Starting training')\n\n###--- REPLACE THE ??? BELOW WITH train_X AND THEN train_Y ---###\ntraining_stats = model.fit(???, ???, batch_size = 1, epochs = 24, verbose = 0)\n###\n\nprint('Training finished')\nprint('Training Evaluation: loss = %0.3f, accuracy = %0.2f%%'\n      %(training_stats.history['loss'][-1], 100 * training_stats.history['acc'][-1]))",
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Starting training\nTraining finished\nTraining Evaluation: loss = 0.193, accuracy = 95.00%\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# We can plot our training statistics to see how it developed over time\n\n###--- REPLACE THE ???s BELOW WITH 'acc' AND THEN 'loss' (INCLUDING THE QUOTES) ---###\naccuracy, =graph.plot(training_stats.history[???],label='Accuracy')\ntraining_loss, =graph.plot(training_stats.history[???],label='Training Loss')\n###\n\ngraph.legend(handles=[accuracy,training_loss])\nloss = np.array(training_stats.history['loss'])\nxp = np.linspace(0, loss.shape[0], 10 * loss.shape[0])\ngraph.plot(xp, np.full(xp.shape, 1), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.plot(xp, np.full(xp.shape, 0), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 6\n\nNow that it's trained, let's see how it performs on our test data! It's important to test a model on data that it has never seen before, to make sure it doesn't overfit. Now let's evaluate it against the test set:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? BELOW WITH test_X AND test_Y ---###\nevaluation = model.evaluate(???, ???, verbose=0)\n###\n\nprint('Test Set Evaluation: loss = %0.6f, accuracy = %0.2f' %(evaluation[0], 100*evaluation[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It seems to be very accurate with the random seed that we set, but let's see how it predicts something completely new and unclassified! Come up with a new sample of the format `[age, weight, height]` to test it with."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "###--- REPLACE ??? BELOW WITH A NEW SAMPLE VECTOR, e.g. [9, 7, 7] ---###\n# [age, weight, height]\nnew_sample = [???, ???, ???]\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Let's have a look at where our new sample sits in the feature space.\n\n# Plots out the age-weight relationship\n\n###--- REPLACE THE ???s BELOW WITH new_sample ---###\ngraph.plot(???[0], ???[1], 'ko', marker='x')\n###\n\ngraph.scatter(train_X[:,0], train_X[:,1], c = target[:160])\ngraph.title('samples by age and weight')\ngraph.xlabel('age')\ngraph.ylabel('weight')\ngraph.show()\n\n# Plot out the age-height relationship\n\n###--- REPLACE THE ???s BELOW WITH new_sample ---###\ngraph.plot(???[0], ???[2], 'ko', marker='x')\n###\n\ngraph.scatter(train_X[:,0], train_X[:,2], c = target[:160])\ngraph.title('samples by age and height')\ngraph.xlabel('age')\ngraph.ylabel('height')\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Looks alright? Now let's see what breed of dog the model says it is!"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ???s BELOW WITH new_sample ---###\npredicted = model.predict(np.array([???]))\nprint('Breed prediction for %s:' %(new_sample))\n###\n\nprint(np.around(predicted[0],2))\nprint('Breed %s, with %i%% certainty.' %(np.argmax(predicted), np.round(100 * predicted[:, np.argmax(predicted)][0])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Breed `0` should be blue, breed `1` should be green, and breed `2` should be red. How does the model's prediction compare to how you would pick this new sample's class?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Conclusion\n\nWe've built a simple neural network to help us predict dog breeds! In the next exercise we'll look into neural networks with a bit more depth, and at the factors that influence how well it learns."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}