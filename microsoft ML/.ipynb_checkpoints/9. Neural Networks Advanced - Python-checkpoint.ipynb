{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Exercise 9 - Advanced Neural Networks\n==========\n\nThere are many factors that influence how well a neural network might perform. AI practitioners tend to play around with the structure of the hidden layers, the activation functions used, and the optimisation function.\n\nIn this exercise we will look at how changing these parameters impacts the accuracy performance of our network."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 1\n------\n\nIn this exercise we will utilise the dog dataset as in exercise 8, building on what we learnt before and switching up the features of the network to try and improve performance.\n\nLet's start by opening up our data and setting up our train and test sets."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport tensorflow as tf\nimport keras\nprint('keras using %s backend'%keras.backend.backend())\n# Sets up the graphing configuration\nimport matplotlib.pyplot as graph\n%matplotlib inline\ngraph.rcParams['figure.figsize'] = (15,5)\ngraph.rcParams[\"font.family\"] = 'DejaVu Sans'\ngraph.rcParams[\"font.size\"] = '12'\ngraph.rcParams['image.cmap'] = 'rainbow'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This gets our data ready\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Load the data\ndata = pd.read_csv('Data/dog_data.csv')\n\n# Separate out the features\nfeatures = data.drop(['breed'], axis = 1)\n\n# Sets the target one-hot vectors\ntarget = OneHotEncoder(sparse = False).fit_transform(np.transpose([data['breed']]))\n\n# Take the first 4/5 of the data and assign it to training\ntrain_X = features.values[:160]\ntrain_Y = target[:160]\n\n# Take the last 1/5 of the data and assign it to testing\ntest_X = features.values[160:]\ntest_Y = target[160:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 2\n------\n\nThe box below contains methods to help us quickly change the structure. Don't edit them - just run the box.\n\nThe __train_network__ method allows us to change:\n* the number of layers\n* the activation functions the layers use\n* the optimizer of the model\n* the number of training cycles for the model (__epochs__)\n\nThe plot_acc and bar_acc just plot our models so we can easily see how well they do.\n\nDon't worry about the code - it is simply to make the next steps easier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Below are a few helper methods. Do not edit these.\n\ndef train_network(structure, activation, optimizer, epochs):\n    # Here we set a randomisation seed for replicatability.\n    np.random.seed(6)\n    \n    # This initialises the model\n    model = keras.models.Sequential()\n    \n    # This is our input + the first hidden layer 1\n    model.add(keras.layers.Dense(units = structure[1], input_dim = structure[0], activation = activation)) \n    \n    # Hidden layer 2, if not ignored (of size 0)\n    if structure[2] > 0:\n        model.add(keras.layers.Dense(units = structure[2], activation = activation))\n        \n    # Output layer\n    model.add(keras.layers.Dense(units=structure[-1], activation = tf.nn.softmax))\n    \n    # Compiles the model with parameters\n    model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    \n    # This tells the us training has started, so we know that it's actually running\n    print('training... ', end = '')\n    \n    # This trains the network\n    training_stats = model.fit(train_X, train_Y, batch_size = 1, epochs = epochs, verbose = 0)\n    \n    # Results!\n    print('train_acc: %0.3f, test_acc: %0.3f' %(training_stats.history['acc'][-1], \n                                                model.evaluate(test_X, test_Y, verbose = 0)[1]))\n    \n    # This returns the results and the model for use outside the function\n    return training_stats, model\n\n# Plots our evaluations in a line graph to see how they compare\ndef plot_acc(train_acc, test_acc, title):\n    # Plots the training and testing accuracy lines\n    training_accuracy, = graph.plot(train_acc, label = 'Training Accuracy')\n    testing_accuracy, = graph.plot(test_acc, label = 'Testing Accuracy')\n    graph.legend(handles = [training_accuracy, testing_accuracy])\n    \n    # Plots guide lines along y = 0 and y = 1 to help visualise\n    xp = np.linspace(0, train_acc.shape[0] - 1, 10 * train_acc.shape[0])\n    graph.plot(xp, np.full(xp.shape, 1), c = 'k', linestyle = ':', alpha = 0.5)\n    graph.plot(xp, np.full(xp.shape, 0), c = 'k', linestyle = ':', alpha = 0.5)\n    \n    graph.xticks(range(0, train_acc.shape[0]), range(1, train_acc.shape[0] + 1))\n    graph.ylim(0,1)\n    graph.title(title)\n    \n    graph.show()\n\n# Plots our evaluations in a bar chart to see how they compare\ndef bar_acc(train_acc, test_acc, title, xticks):\n    index = range(1, train_acc.shape[0] + 1)\n    \n    # Plots the training and testing accuracy bars\n    training_accuracy = graph.bar(index, train_acc, 0.4, align = 'center')\n    testing_accuracy = graph.bar(index, test_acc, 0.4, align = 'edge')\n    graph.legend((training_accuracy[0], testing_accuracy[0]), ('Training Accuracy', 'Testing Accuracy'))\n    \n    graph.xticks(index, xticks)\n    graph.title(title)\n    \n    graph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 3\n------\n\nLet's first look at how different layer sizes impact performance.\n\nLet's look at a network with just one hidden layer. We'll see how it performs with 1 to 10 nodes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Initialises empty arrays into which to append new values.\ntrain_acc = np.empty((0))\ntest_acc = np.empty((0))\n\nfor hidden1 in range (1,11):\n    print('Evaluating model with %i hidden neurons... ' %hidden1, end = '')\n\n###--- REPLACE THE ???'s BELOW WITH hidden1 ---###\n    training_stats, model = train_network(structure = [3, ???, ???, 3], \n                                          activation = 'relu', optimizer = 'SGD', epochs = 12)\n###\n    \n    train_acc = np.append(train_acc, training_stats.history['acc'][-1])\n    test_acc = np.append(test_acc, model.evaluate(test_X, test_Y, verbose = 0)[1])\n\n\n###--- REPLACE THE ???s BELOW WITH train_acc AND THEN test_acc ---###\nplot_acc(train_acc, test_acc, 'hidden layer size performance comparison')\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So, experimenting with different sizes of hidden layers can dramatically improve your results.\n\nStep 4\n------\n\nNow we'll look at how different activation functions impact the performance.\n\nThere's lots we will try, just remember it is common to try both `relu` and `tanh` first."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_acc = np.empty((0))\ntest_acc = np.empty((0))\n\n# Makes a list of the activation functions we wish to compare\nactivation_functions = ['elu', 'selu', 'relu', 'tanh', 'sigmoid', \n                        'hard_sigmoid', 'softplus', 'softsign', 'linear']\n\nfor activation in activation_functions:\n    print('Evaluating model with %s hidden layer activation function... ' %activation, end = '')\n\n###--- REPLACE THE ??? BELOW WITH activation ---###\n    training_stats, model = train_network(structure = [3, 4, 2, 3],\n                                          activation = ???, optimizer = 'SGD', epochs = 12)\n###\n    \n    train_acc = np.append(train_acc, training_stats.history['acc'][-1])\n    test_acc = np.append(test_acc, model.evaluate(test_X, test_Y, verbose=0)[1])\n    \n###--- REPLACE THE ??? BELOW WITH activation_functions ---###\nbar_acc(train_acc, test_acc, 'activation function performance comparison using (4,2) hidden layer', ???)\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There's quite a lot of variance there. It's always good to quickly test different activation functions first.\n\nNext, lets try changing the shape of the hidden layers."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_acc = np.empty((0))\ntest_acc = np.empty((0))\n\nactivation_functions = ['elu', 'selu', 'relu', 'tanh', 'sigmoid',\n                        'hard_sigmoid', 'softplus', 'softsign', 'linear']\n\nfor activation in activation_functions:\n    print('Evaluating model with %s hidden layer activation function... ' %activation, end='')\n    \n# The ???'s below will change the size of the hidden layers. Lets try changing them both to 3 for now\n# (but you can have a play around with different numbers if you want)\n###--- REPLACE THE ???s BELOW WITH 3 ---###\n    training_stats, model = train_network(structure = [3, ???, ???, 3], \n                                          activation = activation, optimizer = 'SGD', epochs = 12)\n###\n    \n    train_acc = np.append(train_acc, training_stats.history['acc'][-1])\n    test_acc = np.append(test_acc, model.evaluate(test_X, test_Y, verbose=0)[1])\n    \nbar_acc(train_acc, test_acc, 'activation function performance comparison using (3,3) hidden layer', activation_functions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 5\n-----\n\nThe __optimisation function__ is the last major parameter of the network architecture. It changes how the network is trained - so it can have a __very large impact on training time and end performance__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_acc = np.empty((0))\ntest_acc = np.empty((0))\n\n# This is a list of the optimisation functions for us to compare\noptimization_functions = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n                          'Adam', 'Adamax', 'Nadam']\n\nfor optimizer in optimization_functions:\n    print('Evaluating model with %s optimizer... ' %optimizer, end='')\n    \n    \n# The ??? below is where we specify the optimizer in the code    \n###--- REPLACE THE ??? BELOW WITH optimizer ---###\n    training_stats, model = train_network(structure = [3, 4, 2, 3],\n                                          activation = 'relu', optimizer = ???, epochs = 12)\n###\n\n# This is recording our data for the plot\n    train_acc = np.append(train_acc, training_stats.history['acc'][-1])\n    test_acc = np.append(test_acc, model.evaluate(test_X, test_Y, verbose=0)[1])\n\n# And now, the plot!    \nbar_acc(train_acc, test_acc, 'optimizer performance comparison using (4,2) hidden layer', optimization_functions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 6\n-------\n\nLet's try to combine what we've seen above and try to create a neural network that performs better than what we made in exercise 7, where we used the structure `[3,4,2,3]`, the activation function `relu`, and the optimiser `SGD` (Stochastic Gradient Descent)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ???s BELOW WITH PARAMETERS TO TEST A NEW NEURAL NETWORK ---###\nstructure = ??? # e.g. [3,4,2,3]\nactivation = ??? # e.g. 'relu', 'softsign', 'tanh', 'elu', 'selu', 'softplus', 'linear'\noptimizer = ??? # e.g. 'SGD', 'adam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adamax', 'Nadam'\n###\n\ntraining_stats, model = train_network(structure, activation, optimizer, epochs = 24)\n\n# We can plot our training statistics to see how it developed over time\naccuracy, = graph.plot(training_stats.history['acc'], label = 'Accuracy')\ntraining_loss, = graph.plot(training_stats.history['loss'], label = 'Training Loss')\ngraph.legend(handles = [accuracy, training_loss])\nloss = np.array(training_stats.history['loss'])\nxp = np.linspace(0, loss.shape[0], 10 * loss.shape[0])\ngraph.plot(xp, np.full(xp.shape, 1), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.plot(xp, np.full(xp.shape, 0), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How does it look? Were we able to beat the other network? Try out a number of different configurations to see how they perform!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Conclusion\n-------\n\nWe've compared how different neural network architecture parameters influence accuracy performance, and we've tried to combine them in such a way that we maximise this performance."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}